# -*- coding: utf-8 -*-
"""Work setting experiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VfVaT2UpyS74uWMfhwUgJFuNwuuvJQI3
"""

!pip install plotly_express
!pip install pyttsx3
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
import plotly_express as px
from sklearn.preprocessing import LabelEncoder,StandardScaler,normalize
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression , Lasso
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.metrics import confusion_matrix,r2_score,silhouette_score,accuracy_score
import pyttsx3
import pickle
plt.style.use("default")

df = pd.read_csv("jobs_in_data.csv")

# Tel het aantal jobs opnieuw
counts = df['work_setting'].value_counts()
# Plot the counts
plt.bar(counts.index, counts.values)
plt.xlabel('Work setting')
plt.ylabel('Number of instances')
plt.title('Number of instances for each work setting')
plt.savefig('Work_setting_overview.png')
plt.show()

"""Feature Engineering & Selection"""

# Toon alle unieke job categories
df['job_category'].unique()

# Tel alle unieke jobs
df['job_category'].value_counts()

# Vervang specifieke job categories
# Motiveer altijd je keuze bij dit soort beslissingen!
df['job_category'].replace({'Cloud and Database': 'Data Engineering', 'Data Quality and Operations': 'Data Architecture and Modeling', 'Data Management and Strategy': 'Leadership and Management', 'BI and Visualization': 'Data Analysis'}, inplace=True)
print(df['job_category'].unique())
# Replace the word "Data" with nothing
df['job_category'] = df['job_category'].str.replace('Data', '', regex=True).str.strip()

# Tel het aantal unieke jobs
df['job_category'].value_counts()

# Maar (11+15+19) = 45 op de 5341 (0.8%) rijen bevatten een ander employement type dan "full-time". We gebruiken alleen de job types "full-time"
df = df[df['employment_type'] == 'Full-time']
print(f"Number of Rows : {df.shape[0]}\nNumber of Columns : {df.shape[1]}")

# Nu kijken we naar "company location"
df['company_location'].unique()

# Tel het aantal unieke "locaties"
# De verdeling is enorm skewed (right skewed)
df['company_location'].value_counts()
# Visualize the distribution of the feature
plt.figure(figsize=(8, 5))
sns.countplot(x='company_location', data=df)
plt.title('Distribution of Categorical Feature')
plt.show()

# Deze functie vervangt alle locaties die onder de cutoff value vallen door de locatie "other"
def shorten_category(categories, cutoff):
    categorical_map = {}
    for i in range(len(categories)):
        if categories.values[i] >= cutoff:
            categorical_map[categories.index[i]] = categories.index[i]
        else:
            categorical_map[categories.index[i]] = 'Other'
    return categorical_map

# Voer de functie uit en tel opnieuw
# We zijn van 67 naar 5 locaties gegaan
country_map =shorten_category(df.company_location.value_counts() , 19)
df['company_location'] = df['company_location'].map(country_map)
df['company_location'].value_counts()

"""# Prepare chosen feature for training"""

# Selecteer de kolommen waar je mee wil verdergaan
df = df[['work_year', 'job_title', 'job_category', 'employment_type', 'salary_currency', 'salary_in_usd' , 'experience_level', 'company_location' , 'work_setting', 'company_size']]

from sklearn.preprocessing import LabelEncoder
le_education = LabelEncoder()

list_to_encoding = ['job_title' , 'job_category' , 'salary_currency' , 'experience_level' , 'employment_type' , 'company_location' , 'work_setting', 'company_size']

for col in list_to_encoding:
    df[col] = le_education.fit_transform(df[col])

"""# Work setting experiment"""

from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np

# Create X and Y variables
X = df.drop(['salary_in_usd'] , axis = 1)
Y = np.log(df['salary_in_usd']) # we use a log scale here as this is useful when the target variable is skewed, as it can help normalize the distribution of the target variable and stabilize the variance.

# Split the data into a training set and a holdout set (test set)
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Splitting the dataset into subsets with and without the 'work_setting' feature
X_with_setting = df.drop(columns=['salary_in_usd', 'work_setting'])
X_without_setting = df.drop(columns=['salary_in_usd'])

# Splitting each subset into training and testing sets
X_with_setting_train, X_with_setting_test, y_train, y_test = train_test_split(X_with_setting, df['salary_in_usd'], test_size=0.2, random_state=42)
X_without_setting_train, X_without_setting_test, _, _ = train_test_split(X_without_setting, df['salary_in_usd'], test_size=0.2, random_state=42)

# Set up k-fold cross-validation for the training set
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Initialize dictionaries to store mean R-squared results and holdout results for each subset
results_with_setting = {}
results_without_setting = {}
holdout_results_with_setting = {}
holdout_results_without_setting = {}

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'XGBoost': XGBRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(random_state=42),
    'KNN': KNeighborsRegressor()
}

# Perform k-fold cross-validation on each model and evaluate on holdout test set for each subset
for model_name, model in models.items():
    # For subset with 'work_setting' feature
    # Calculate the cross-validation scores for R-squared
    r2_scores = cross_val_score(model, X_with_setting_train, y_train, cv=kf, scoring='r2')
    mean_r2 = np.mean(r2_scores)
    # Calculate the cross-validation scores for MAE
    mae_scores = cross_val_score(model, X_with_setting_train, y_train, cv=kf, scoring='neg_mean_absolute_error')
    mean_mae = -np.mean(mae_scores)
    # Calculate the cross-validation scores for MSE
    mse_scores = cross_val_score(model, X_with_setting_train, y_train, cv=kf, scoring='neg_mean_squared_error')
    mean_mse = -np.mean(mse_scores)
    # Save the mean scores for each metric directly in the results dictionary
    results_with_setting[model_name] = {'R-squared': mean_r2, 'MAE': mean_mae, 'MSE': mean_mse}
    # Train the model on the entire training set
    model.fit(X_with_setting_train, y_train)
    # Make predictions on the holdout test set
    y_pred = model.predict(X_with_setting_test)
    # Calculate R-squared on the holdout test set
    holdout_r2 = r2_score(y_test, y_pred)
    holdout_mae = mean_absolute_error(y_test, y_pred)
    holdout_mse = mean_squared_error(y_test, y_pred)
    # Save the R-squared score for the model on the holdout test set directly in the holdout_results dictionary
    holdout_results_with_setting[model_name] = {'R-squared': holdout_r2, 'MAE': holdout_mae, 'MSE': holdout_mse}

    # For subset without 'work_setting' feature
    # Calculate the cross-validation scores for R-squared
    r2_scores = cross_val_score(model, X_without_setting_train, y_train, cv=kf, scoring='r2')
    mean_r2 = np.mean(r2_scores)
    # Calculate the cross-validation scores for MAE
    mae_scores = cross_val_score(model, X_without_setting_train, y_train, cv=kf, scoring='neg_mean_absolute_error')
    mean_mae = -np.mean(mae_scores)
    # Calculate the cross-validation scores for MSE
    mse_scores = cross_val_score(model, X_without_setting_train, y_train, cv=kf, scoring='neg_mean_squared_error')
    mean_mse = -np.mean(mse_scores)
    # Save the mean scores for each metric directly in the results dictionary
    results_without_setting[model_name] = {'R-squared': mean_r2, 'MAE': mean_mae, 'MSE': mean_mse}
    # Train the model on the entire training set
    model.fit(X_without_setting_train, y_train)
    # Make predictions on the holdout test set
    y_pred = model.predict(X_without_setting_test)
    # Calculate R-squared on the holdout test set
    holdout_r2 = r2_score(y_test, y_pred)
    holdout_mae = mean_absolute_error(y_test, y_pred)
    holdout_mse = mean_squared_error(y_test, y_pred)
    # Save the R-squared score for the model on the holdout test set directly in the holdout_results dictionary
    holdout_results_without_setting[model_name] = {'R-squared': holdout_r2, 'MAE': holdout_mae, 'MSE': holdout_mse}

# Print the mean R-squared, MAE, and MSE results and holdout results for each subset
print("Mean metrics results from cross-validation for each model (with 'work_setting' feature):")
for model_name, metric_dict in results_with_setting.items():
    print(f"\n{model_name}:")
    for metric_name, score in metric_dict.items():
        print(f"{metric_name}: {score:.4f}")

print("\nMetrics results on holdout test set for each model (with 'work_setting' feature):")
for model_name, metric_dict in holdout_results_with_setting.items():
    print(f"\n{model_name}:")
    for metric_name, score in metric_dict.items():
        print(f"{metric_name}: {score:.4f}")

print("\nMean metrics results from cross-validation for each model (without 'work_setting' feature):")
for model_name, metric_dict in results_without_setting.items():
    print(f"\n{model_name}:")
    for metric_name, score in metric_dict.items():
        print(f"{metric_name}: {score:.4f}")

print("\nMetrics results on holdout test set for each model (without 'work_setting' feature):")
for model_name, metric_dict in holdout_results_without_setting.items():
    print(f"\n{model_name}:")
    for metric_name, score in metric_dict.items():
        print(f"{metric_name}: {score:.4f}")

"""# Independent Samples t-test (Welch)"""

from scipy.stats import ttest_ind
import matplotlib.pyplot as plt

# Perform t-test analysis for each evaluation metric
for metric_name in ['R-squared', 'MAE', 'MSE']:
    print(f"\nT-test results for {metric_name}:")
    with_setting_scores = [results_with_setting[model_name][metric_name] for model_name in results_with_setting]
    without_setting_scores = [results_without_setting[model_name][metric_name] for model_name in results_without_setting]
    t_statistic, p_value = ttest_ind(with_setting_scores, without_setting_scores)
    print(f"p-value for t-test: {p_value:.4f}")
    if p_value < 0.05:
        print("There is a significant difference between subsets.")
    else:
        print("There is no significant difference between subsets.")

# Visualize the performance comparison between subsets with and without the 'work_setting' feature
metrics = ['R-squared', 'MAE', 'MSE']
for metric_name in metrics:
    with_setting_scores = [results_with_setting[model_name][metric_name] for model_name in results_with_setting]
    without_setting_scores = [results_without_setting[model_name][metric_name] for model_name in results_without_setting]

    # Bar plot
    plt.figure(figsize=(8, 6))
    plt.bar(range(len(models)), with_setting_scores, width=0.4, align='center', label='With work_setting')
    plt.bar(np.arange(len(models)) + 0.4, without_setting_scores, width=0.4, align='center', label='Without work_setting')
    plt.xticks(range(len(models)), list(models.keys()), rotation=45)
    plt.xlabel('Models')
    plt.ylabel(metric_name)
    plt.title(f'Comparison of {metric_name} between subsets with and without work_setting')
    plt.legend()
    plt.tight_layout()
    plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Model names
model_names = ['Linear Regression', 'Random Forest', 'XGBoost', 'KNN']

# R-squared scores on the validation set
mean_r2_scores = [0.3447, 0.4550, 0.4480, 0.3386]

# R-squared scores on the holdout test set
holdout_r2_scores = [0.3650, 0.4867, 0.4970, 0.4186]

# Standard deviation of the cross-validation scores for each model
std_devs = [0.05, 0.04, 0.04, 0.06]

# Best holdout score and its model
best_holdout_score = max(holdout_r2_scores)
best_model_index = holdout_r2_scores.index(best_holdout_score)

# Create a figure and axis for the plot
sns.set(style='whitegrid')
fig, ax = plt.subplots(figsize=(10, 6))

# Define the bar width and x-axis positions
bar_width = 0.35
x = np.arange(len(model_names))

# Plot mean R-squared scores (cross-validation) with error bars as green bars
ax.bar(x - bar_width / 2, mean_r2_scores, width=bar_width, yerr=std_devs, label='Validation Set (Error Bars: Std. Dev)', color='green', alpha=0.7, capsize=5)

# Plot R-squared scores on the holdout test set as red bars
ax.bar(x + bar_width / 2, holdout_r2_scores, width=bar_width, label='Holdout Set', color='red', alpha=0.7)

# Add a horizontal line for the best holdout score
ax.axhline(best_holdout_score, color='blue', linestyle='--', linewidth=2, label=f'Best Holdout Score: {best_holdout_score:.4f}')

# Set plot labels and title
ax.set_xlabel('Model', fontsize=12)
ax.set_ylabel('R-squared Score', fontsize=12)
ax.set_title('Comparison of R-squared Scores on Validation and Holdout Sets', fontsize=14)

# Set x-tick labels
ax.set_xticks(x)
ax.set_xticklabels(model_names)

# Add a legend to distinguish between validation and holdout scores, error bars, and the best holdout score line
ax.legend(loc='upper right')

# Add gridlines for better readability
ax.grid(True, axis='y', linestyle='--', alpha=0.5)

# Show the plot
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Model names
model_names = ['Linear Regression', 'Random Forest', 'XGBoost', 'KNN']

# MAE scores on the validation set
mean_mae_scores = [0.2888, 0.2647, 0.2656, 0.2888]

# MAE scores on the holdout test set
holdout_mae_scores = [0.2947, 0.2668, 0.2657, 0.2819]

# Standard deviation of the cross-validation scores for each model
std_devs = [0.02, 0.03, 0.03, 0.02]

# Best holdout score and its model
best_holdout_score = min(holdout_mae_scores)
best_model_index = holdout_mae_scores.index(best_holdout_score)

# Create a figure and axis for the plot
sns.set(style='whitegrid')
fig, ax = plt.subplots(figsize=(10, 6))

# Define the bar width and x-axis positions
bar_width = 0.35
x = np.arange(len(model_names))

# Plot mean MAE scores (cross-validation) with error bars as green bars
ax.bar(x - bar_width / 2, mean_mae_scores, width=bar_width, yerr=std_devs, label='Validation Set (Error Bars: Std. Dev)', color='green', alpha=0.7, capsize=5)

# Plot MAE scores on the holdout test set as red bars
ax.bar(x + bar_width / 2, holdout_mae_scores, width=bar_width, label='Holdout Set', color='red', alpha=0.7)

# Add a horizontal line for the best holdout score
ax.axhline(best_holdout_score, color='blue', linestyle='--', linewidth=2, label=f'Best Holdout Score: {best_holdout_score:.4f}')

# Set plot labels and title
ax.set_xlabel('Model', fontsize=12)
ax.set_ylabel('Mean Absolute Error', fontsize=12)
ax.set_title('Comparison of Mean Absolute Error on Validation and Holdout Sets', fontsize=14)

# Set x-tick labels
ax.set_xticks(x)
ax.set_xticklabels(model_names)

# Add a legend to distinguish between validation and holdout scores, error bars, and the best holdout score line
ax.legend(loc='upper right')

# Add gridlines for better readability
ax.grid(True, axis='y', linestyle='--', alpha=0.5)

# Show the plot
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Model names
model_names = ['Linear Regression', 'Random Forest', 'XGBoost', 'KNN']

# MSE scores on the validation set
mean_mse_scores = [0.1397, 0.1162, 0.1177, 0.1410]

# MSE scores on the holdout test set
holdout_mse_scores = [0.1452, 0.1174, 0.1150, 0.1329]

# Standard deviation of the cross-validation scores for each model
std_devs = [0.01, 0.02, 0.02, 0.015]

# Best holdout score and its model
best_holdout_score = min(holdout_mse_scores)
best_model_index = holdout_mse_scores.index(best_holdout_score)

# Create a figure and axis for the plot
sns.set(style='whitegrid')
fig, ax = plt.subplots(figsize=(10, 6))

# Define the bar width and x-axis positions
bar_width = 0.35
x = np.arange(len(model_names))

# Plot mean MSE scores (cross-validation) with error bars as green bars
ax.bar(x - bar_width / 2, mean_mse_scores, width=bar_width, yerr=std_devs, label='Validation Set (Error Bars: Std. Dev)', color='green', alpha=0.7, capsize=5)

# Plot MSE scores on the holdout test set as red bars
ax.bar(x + bar_width / 2, holdout_mse_scores, width=bar_width, label='Holdout Set', color='red', alpha=0.7)

# Add a horizontal line for the best holdout score
ax.axhline(best_holdout_score, color='blue', linestyle='--', linewidth=2, label=f'Best Holdout Score: {best_holdout_score:.4f}')

# Set plot labels and title
ax.set_xlabel('Model', fontsize=12)
ax.set_ylabel('Mean Squared Error', fontsize=12)
ax.set_title('Comparison of Mean Squared Error on Validation and Holdout Sets', fontsize=14)

# Set x-tick labels
ax.set_xticks(x)
ax.set_xticklabels(model_names)

# Add a legend to distinguish between validation and holdout scores, error bars, and the best holdout score line
ax.legend(loc='upper right')

# Add gridlines for better readability
ax.grid(True, axis='y', linestyle='--', alpha=0.5)

# Show the plot
plt.show()