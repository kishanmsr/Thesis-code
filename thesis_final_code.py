# -*- coding: utf-8 -*-
"""Thesis final code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OUHd-ja3ooizd0VKUdK1wmIsDpTAibgd
"""

!pip install plotly_express
!pip install pyttsx3
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
import plotly_express as px
from sklearn.preprocessing import LabelEncoder,StandardScaler,normalize
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression , Lasso
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.metrics import confusion_matrix,r2_score,silhouette_score,accuracy_score
import pyttsx3
import pickle
plt.style.use("default")

data = pd.read_csv ("jobs_in_data.csv")
#data = data.drop(['salary'], axis=1)
data

data.describe()

import matplotlib.pyplot as plt

plt.hist(data['employment_type'])
plt.title('Employment Type Distribution')
plt.xlabel('Employment Type')
plt.ylabel('Frequency')

# Save the plot as an image file
plt.savefig('employment_type_histogram.png')

# Show the plot (optional)
plt.show()

# Save the plot as an image file
plt.savefig('Employment_type_histogram.png')

# for column in data.columns:
#     plt.figure(figsize=(8, 5))
#     sns.boxplot(x=data[column])
#     plt.title(f'Boxplot of {column}')
#     plt.xticks(rotation=90, fontsize=20)
#     plt.show()

from matplotlib import pyplot as plt
# Assuming data is already loaded
data['salary_in_usd'].plot(kind='hist', bins=30, title='Histogram of Salary in USD', edgecolor='black')
plt.xlabel('Salary in USD')
plt.ylabel('Frequency')

# Save the plot as an image file
plt.savefig('salary_histogram.png')

# Show the plot (optional)
plt.show()

data.info()

work_year = []
for i in data['work_year']:
    work_year.append(i)
unique = []
for i in work_year:
    if i not in unique:
        unique.append(i)
print(unique)

sns.lineplot(x =data['work_year'] , y = data['salary_in_usd'],data=data , linewidth = 3)

plt.hist(data['employment_type'])

sns.lineplot(x =data['work_year'] , y = data['salary_currency'],data=data , linewidth = 3)

sns.barplot(x=data['experience_level'].index, y=data['experience_level'].values, palette='viridis')

fig = px.choropleth(data.groupby('employee_residence')['salary_in_usd'].mean().reset_index(name='Average Salary'),
                    locations='employee_residence',
                    locationmode='country names',
                    color= 'Average Salary',
                    hover_name='employee_residence',
                    color_continuous_scale='plasma')
fig.update_geos(projection_type="natural earth", showcoastlines=True)
fig.update_layout(title_text='World Map - Average Salary')
fig.show()

plt.hist(data['company_size'])

sns.barplot(x=data['salary_currency'].index, y=data['salary_currency'].values, palette='viridis')

# Creating Subplots
# fig,axes = plt.subplots(nrows = 6, ncols = 2, figsize = (15, 30))
# axes = axes.flat

# color_cycle = list(plt.rcParams['axes.prop_cycle'])
# num_colors = len(color_cycle)

# for i, num_col in enumerate(data):
#     sns.histplot(data,
#                x = num_col,
#                stat = 'count',
#                kde = True,
#                color = color_cycle[i % num_colors]["color"],
#                line_kws = {'linewidth': 2,
#                            'linestyle':'dashed'},
#                alpha = 0.4,
#                ax = axes[i])
#     sns.rugplot(data,
#               x = num_col,
#               color = color_cycle[i % num_colors]["color"],
#               ax = axes[i], alpha = 0.7)
#     axes[i].set_xlabel(" ")
#     axes[i].set_ylabel("Count", fontsize = 7, fontweight = 'bold', color = 'black')
#     axes[i].set_title(num_col, fontsize = 8, fontweight = 'bold', color = 'black')
#     axes[i].tick_params(labelsize = 6)

# fig.suptitle('Distribution of Dataset', fontsize = 12, fontweight = 'bold', color = 'darkred', y = 0.92)
# fig.tight_layout()
# fig.subplots_adjust(top = 0.9)
# fig.show()

data.nunique()

columns = list(data.columns)
columns

objects = data.select_dtypes(include=['object'])
objects

ints = data.select_dtypes(exclude=['object'])
ints

LE = LabelEncoder()
for i in (objects):
    objects[i] = LE.fit_transform(objects[i])
data = pd.concat([ints,objects],axis=1)
data

sns.heatmap(data)

data.describe()

data.corr()

plt.figure(figsize=(60, 40))
matrix = np.triu(data.corr())
sns.heatmap(data.corr(), annot=True, linewidth=.10, mask=matrix, annot_kws={"fontsize": 60})
plt.title('Correlation between features in dataset', fontsize=60)
plt.xticks(rotation=90, fontsize=60)
plt.yticks(rotation=0, fontsize=60)
plt.savefig('Correlation_Matrix.png')  # Save the figure as an image
plt.show()

# Check NaN Value
# df.isna().sum()
null_counts = data.isnull().sum()

# Plotting
plt.figure(figsize=(20, 16))  # Increase figure size
plt.bar(null_counts.index, null_counts.values, color='skyblue')
plt.title('Null Values in the Dataset', fontsize=20)
plt.xlabel('Features')
plt.ylabel('Null Count')
plt.xticks(rotation=45, ha='right')  # Rotate x-labels and align to right

# Save the figure with higher resolution
plt.savefig('Null_count.png')

# Show the plot
plt.show()

df = pd.read_csv("jobs_in_data.csv")

# Tel het aantal jobs opnieuw
counts = df['work_setting'].value_counts()
# Plot the counts
plt.bar(counts.index, counts.values)
plt.xlabel('Work setting')
plt.ylabel('Number of instances')
plt.title('Number of instances for each work setting')
plt.savefig('Work_setting_overview.png')
plt.show()

"""Feature Engineering & Selection"""

# Toon alle unieke job categories
df['job_category'].unique()

# Tel alle unieke jobs
df['job_category'].value_counts()

# Vervang specifieke job categories
# Motiveer altijd je keuze bij dit soort beslissingen!
df['job_category'].replace({'Cloud and Database': 'Data Engineering', 'Data Quality and Operations': 'Data Architecture and Modeling', 'Data Management and Strategy': 'Leadership and Management', 'BI and Visualization': 'Data Analysis'}, inplace=True)
print(df['job_category'].unique())
# Replace the word "Data" with nothing
df['job_category'] = df['job_category'].str.replace('Data', '', regex=True).str.strip()

# Tel het aantal unieke jobs
df['job_category'].value_counts()

# Maar (11+15+19) = 45 op de 5341 (0.8%) rijen bevatten een ander employement type dan "full-time". We gebruiken alleen de job types "full-time"
df = df[df['employment_type'] == 'Full-time']
print(f"Number of Rows : {df.shape[0]}\nNumber of Columns : {df.shape[1]}")

# Nu kijken we naar "company location"
df['company_location'].unique()

# Tel het aantal unieke "locaties"
# De verdeling is enorm skewed (right skewed)
df['company_location'].value_counts()
# Visualize the distribution of the feature
plt.figure(figsize=(8, 5))
sns.countplot(x='company_location', data=df)
plt.title('Distribution of Categorical Feature')
plt.show()

# Deze functie vervangt alle locaties die onder de cutoff value vallen door de locatie "other"
def shorten_category(categories, cutoff):
    categorical_map = {}
    for i in range(len(categories)):
        if categories.values[i] >= cutoff:
            categorical_map[categories.index[i]] = categories.index[i]
        else:
            categorical_map[categories.index[i]] = 'Other'
    return categorical_map

# Voer de functie uit en tel opnieuw
# We zijn van 67 naar 5 locaties gegaan
country_map =shorten_category(df.company_location.value_counts() , 19)
df['company_location'] = df['company_location'].map(country_map)
df['company_location'].value_counts()

"""# Prepare chosen feature for training"""

# Selecteer de kolommen waar je mee wil verdergaan
df = df[['work_year', 'job_title', 'job_category', 'employment_type', 'salary_currency', 'salary_in_usd' , 'experience_level', 'company_location' , 'work_setting', 'company_size']]

from sklearn.preprocessing import LabelEncoder
le_education = LabelEncoder()

list_to_encoding = ['job_title' , 'job_category' , 'salary_currency' , 'experience_level' , 'employment_type' , 'company_location' , 'work_setting', 'company_size']

for col in list_to_encoding:
    df[col] = le_education.fit_transform(df[col])

df.columns

"""# Train model"""

from sklearn.inspection import permutation_importance
from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np

# Create X and Y variables
X = df.drop(['salary_in_usd'] , axis = 1)
Y = np.log(df['salary_in_usd']) # we use a log scale here as this is useful when the target variable is skewed, as it can help normalize the distribution of the target variable and stabilize the variance.

# Split the data into a training set and a holdout set (test set)
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Set up k-fold cross-validation for the training set
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Initialize dictionaries to store mean R-squared results, holdout results, and permutation importance
results = {}
holdout_results = {}
perm_importance_results = {}

# Assuming X_train is a pandas DataFrame
feature_names = X_train.columns.tolist()

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'XGBoost': XGBRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(random_state=42),
    'KNN': KNeighborsRegressor()
}

# Perform k-fold cross-validation on each model and evaluate on holdout test set
for model_name, model in models.items():

    # Calculate the cross-validation scores for R-squared
    r2_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='r2')
    mean_r2 = np.mean(r2_scores)

    # Calculate the cross-validation scores for MAE
    mae_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')
    mean_mae = -np.mean(mae_scores)

    # Calculate the cross-validation scores for MSE
    mse_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')
    mean_mse = -np.mean(mse_scores)

    # Save the mean scores for each metric directly in the results dictionary
    results[model_name] = {'R-squared': mean_r2, 'MAE': mean_mae, 'MSE': mean_mse}

    # Train the model on the entire training set
    model.fit(X_train, y_train)

    # Make predictions on the holdout test set
    y_pred = model.predict(X_test)

    # Calculate R-squared on the holdout test set
    holdout_r2 = r2_score(y_test, y_pred)
    holdout_mae = mean_absolute_error(y_test, y_pred)
    holdout_mse = mean_squared_error(y_test, y_pred)

    # Save the R-squared score for the model on the holdout test set directly in the holdout_results dictionary
    holdout_results[model_name] = {'R-squared': holdout_r2, 'MAE': holdout_mae, 'MSE': holdout_mse}

    # Calculate permutation importance
    perm_importance_result = permutation_importance(model, X_test, y_test, n_repeats=30, random_state=42)
    perm_importance_results[model_name] = perm_importance_result

# Print the mean R-squared, MAE, and MSE results, holdout results, and permutation importance
print("Mean metrics results from cross-validation for each model:")
for model_name, metric_dict in results.items():
    print(f"\n{model_name}:")
    for metric_name, score in metric_dict.items():
        print(f"{metric_name}: {score:.4f}")

print("\nMetrics results on holdout test set for each model:")
for model_name, metric_dict in holdout_results.items():
    print(f"\n{model_name}:")
    for metric_name, score in metric_dict.items():
        print(f"{metric_name}: {score:.4f}")

print("\nPermutation importance results for each model:")
for model_name, perm_importance_result in perm_importance_results.items():
    print(f"\n{model_name}:")
    sorted_idx = perm_importance_result.importances_mean.argsort()
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(feature_names)), perm_importance_result.importances_mean[sorted_idx], xerr=perm_importance_result.importances_std[sorted_idx], align='center')
    plt.yticks(range(len(feature_names)), [feature_names[i] for i in sorted_idx], fontsize = 15)
    plt.xlabel('Feature Importance (loss: R-Squared)', fontsize = 15)
    plt.title(f'Permutation Importance for {model_name}', fontsize = 17)
    plt.show()

"""# Random Forest"""

from sklearn.preprocessing import LabelEncoder

list_to_encode = ['experience_level', 'company_size']

for col in list_to_encode:
    df[col] = le_education.fit_transform(df[col])

from sklearn.preprocessing import OneHotEncoder

# Initialize OneHotEncoder
ohe = OneHotEncoder()

# Columns to one-hot encode
columns_to_encode = ['job_title' , 'job_category' , 'salary_currency' ,
                     'company_location' ,
                     'work_setting', 'employment_type']

# Apply one-hot encoding using pandas get_dummies()
one_hot_encoded_df = pd.get_dummies(df[columns_to_encode])

# Concatenate the new one-hot encoded DataFrame with the original DataFrame
df = pd.concat([df, one_hot_encoded_df], axis=1)

# Drop the original columns since they are now one-hot encoded
df.drop(columns=columns_to_encode, inplace=True)

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# Define a hyperparameter grid for RandomForestRegressor
param_grid = {
    'n_estimators': randint(50, 300),  # Number of trees in the forest
    'max_depth': randint(3, 20),  # Maximum depth of the trees
    'min_samples_split': randint(2, 10),  # Minimum samples to split a node
    'min_samples_leaf': randint(1, 10),  # Minimum samples in a leaf node
    'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider at each split
    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees
}

# Initialize the RandomForestRegressor
rf = RandomForestRegressor(random_state=42)

# Initialize RandomizedSearchCV with the RandomForestRegressor and hyperparameter grid
random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid,
                                   n_iter=100, scoring='neg_mean_squared_error',
                                   cv=5, random_state=42, n_jobs=-1, verbose=1)

# Fit RandomizedSearchCV to the training data
random_search.fit(X_train, y_train)

# Retrieve the best KNeighborsRegressor model
best_rf_model = random_search.best_estimator_

# Evaluate the best model on the holdout test set
y_pred = best_rf_model.predict(X_test)
r2_best_model = r2_score(y_test, y_pred)
mae_best_model = mean_absolute_error(y_test, y_pred)
mse_best_model = mean_squared_error(y_test, y_pred)

# Print the best hyperparameters and evaluation scores of the best model
print("Best hyperparameters for Random Forest Regressor:")
print(random_search.best_params_)
print(f"R-squared on the holdout test set for best model: {r2_best_model:.4f}")
print(f"MAE on the holdout test set for best model: {mae_best_model:.4f}")
print(f"MSE on the holdout test set for best model: {mse_best_model:.4f}")

# Replace the holdout score for the best model with the score of the tuned model on the holdout set
holdout_results["Random Forest"] = {'R-squared': r2_best_model, 'MAE': mae_best_model, 'MSE': mse_best_model}

"""# XGBoost"""

from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# Define a hyperparameter grid for XGBoost
param_grid = {
    'n_estimators': randint(100, 1000),  # Number of trees in the forest
    'max_depth': randint(3, 20),          # Maximum depth of a tree
    'learning_rate': uniform(0.01, 0.3),  # Learning rate
    'subsample': uniform(0.6, 0.4),       # Subsample ratio of the training instances
    'colsample_bytree': uniform(0.6, 0.4),# Subsample ratio of columns when constructing each tree
    'gamma': uniform(0, 5),               # Minimum loss reduction required to make a further partition
    'reg_alpha': uniform(0, 5),           # L1 regularization term on weights
    'reg_lambda': uniform(0, 5),          # L2 regularization term on weights
}

# Initialize the XGBoost Regressor
xgb = XGBRegressor(random_state=42)

# Initialize RandomizedSearchCV with the XGBoost Regressor and hyperparameter grid
random_search = RandomizedSearchCV(estimator=xgb, param_distributions=param_grid, n_iter=100,
                                   scoring='neg_mean_squared_error', cv=5, random_state=42,
                                   n_jobs=-1, verbose=1)

# Fit RandomizedSearchCV to the training data
random_search.fit(X_train, y_train)

# Retrieve the best KNeighborsRegressor model
best_knn_model = random_search.best_estimator_

# Evaluate the best model on the holdout test set
y_pred = best_knn_model.predict(X_test)
r2_best_model = r2_score(y_test, y_pred)
mae_best_model = mean_absolute_error(y_test, y_pred)
mse_best_model = mean_squared_error(y_test, y_pred)

# Print the best hyperparameters and evaluation scores of the best model
print("Best hyperparameters for XGBoost Regressor:")
print(random_search.best_params_)
print(f"R-squared on the holdout test set for best model: {r2_best_model:.4f}")
print(f"MAE on the holdout test set for best model: {mae_best_model:.4f}")
print(f"MSE on the holdout test set for best model: {mse_best_model:.4f}")

# Replace the holdout score for the best model with the score of the tuned model on the holdout set
holdout_results["XGBoost"] = {'R-squared': r2_best_model, 'MAE': mae_best_model, 'MSE': mse_best_model}

import matplotlib.pyplot as plt
import seaborn as sns

# Predict the target values for the test set
y_pred = best_knn_model.predict(X_test)

# Calculate the residuals
residuals = y_test - y_pred

# Create a residual plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_pred, y=residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values', fontsize = 15)
plt.ylabel('Residuals', fontsize = 15)
plt.title('Residual Plot for XGBoost Model', fontsize = 17)
plt.show()

"""# KNN"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# Define the hyperparameter grid for KNeighborsRegressor
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors to use
    'weights': ['uniform', 'distance'],  # Weight function used in prediction
    'algorithm': ['auto', 'ball_tree'],  # Algorithm used to compute the nearest neighbors
    'leaf_size': [10, 20, 30, 40, 50],  # Leaf size passed to BallTree
    'p': [1, 2],  # Power parameter for the Minkowski metric
    'metric': ['euclidean', 'manhattan', 'minkowski']
}


# Initialize the KNN Regressor
knn_regressor = KNeighborsRegressor()

# Initialize RandomizedSearchCV with the KNN Regressor and hyperparameter grid
random_search = RandomizedSearchCV(estimator=knn_regressor,
                                   param_distributions=param_grid, n_iter=100,
                                   cv=5, scoring='neg_mean_squared_error',
                                   random_state=42)

# Fit RandomizedSearchCV to the training data
random_search.fit(X_train, y_train)

# Retrieve the best KNeighborsRegressor model
best_knn_model = random_search.best_estimator_

# Evaluate the best model on the holdout test set
y_pred = best_knn_model.predict(X_test)
r2_best_model = r2_score(y_test, y_pred)
mae_best_model = mean_absolute_error(y_test, y_pred)
mse_best_model = mean_squared_error(y_test, y_pred)

# Print the best hyperparameters and evaluation scores of the best model
print("Best hyperparameters for KNN Regressor:")
print(random_search.best_params_)
print(f"R-squared on the holdout test set for best model: {r2_best_model:.4f}")
print(f"MAE on the holdout test set for best model: {mae_best_model:.4f}")
print(f"MSE on the holdout test set for best model: {mse_best_model:.4f}")

# Replace the holdout score for the best model with the score of the tuned model on the holdout set
holdout_results["KNN"] = {'R-squared': r2_best_model, 'MAE': mae_best_model, 'MSE': mse_best_model}

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Model names
model_names = ['Linear Regression', 'Random Forest', 'XGBoost', 'KNN']

# R-squared scores on the validation set
mean_r2_scores = [0.3352, 0.4563, 0.4535, 0.3643]

# R-squared scores on the holdout test set
holdout_r2_scores = [0.3604, 0.4940, 0.4958, 0.4316]

# Standard deviation of the cross-validation scores for each model
std_devs = [0.05, 0.04, 0.04, 0.06]

# Best holdout score and its model (not provided)
best_holdout_score = max(holdout_r2_scores)
best_model_index = holdout_r2_scores.index(best_holdout_score)

# Create a figure and axis for the plot
sns.set(style='whitegrid')
fig, ax = plt.subplots(figsize=(10, 6))

# Define the bar width and x-axis positions
bar_width = 0.35
x = np.arange(len(model_names))

# Plot mean R-squared scores (cross-validation) with error bars as green bars
bars1 = ax.bar(x - bar_width / 2, mean_r2_scores, width=bar_width, yerr=std_devs, label='Validation Set (Error Bars: Std. Dev)', color='green', alpha=0.7, capsize=5)

# Plot R-squared scores on the holdout test set as red bars
bars2 = ax.bar(x + bar_width / 2, holdout_r2_scores, width=bar_width, label='Holdout Set', color='red', alpha=0.7)

# Add a horizontal line for the best holdout score
ax.axhline(best_holdout_score, color='blue', linestyle='--', linewidth=2, label=f'Best Holdout Score: {best_holdout_score:.4f}')

# Annotate each bar with its value at the bottom
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.4f}',
                    xy=(bar.get_x() + bar.get_width() / 2, 0),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')  # Align annotation to the bottom of the bar

# Set plot labels and title
ax.set_xlabel('Model', fontsize=12)
ax.set_ylabel('R-squared Score', fontsize=12)
ax.set_title('Comparison of R-squared Scores on Validation and Holdout Sets', fontsize=14)

# Set x-tick labels
ax.set_xticks(x)
ax.set_xticklabels(model_names)

# Add a legend to distinguish between validation and holdout scores, error bars, and the best holdout score line
ax.legend(loc='upper right')

# Add gridlines for better readability
ax.grid(True, axis='y', linestyle='--', alpha=0.5)

plt.savefig('Overview_R2.png')

# Show the plot
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Model names
model_names = ['Linear Regression', 'Random Forest', 'XGBoost', 'KNN']

# MAE scores on the validation set
mean_mae_scores = [0.2908, 0.2643, 0.2649, 0.2848]

# MAE scores on the holdout test set
holdout_mae_scores = [0.2958, 0.2661, 0.2666, 0.2795]

# Standard deviation of the cross-validation scores for each model
std_devs = [0.02, 0.03, 0.03, 0.02]

# Best holdout score and its model (not provided)
best_holdout_score = min(holdout_mae_scores)
best_model_index = holdout_mae_scores.index(best_holdout_score)

# Create a figure and axis for the plot
sns.set(style='whitegrid')
fig, ax = plt.subplots(figsize=(10, 6))

# Define the bar width and x-axis positions
bar_width = 0.35
x = np.arange(len(model_names))

# Plot mean MAE scores (cross-validation) with error bars as green bars
bars1 = ax.bar(x - bar_width / 2, mean_mae_scores, width=bar_width, yerr=std_devs, label='Validation Set (Error Bars: Std. Dev)', color='green', alpha=0.7, capsize=5)

# Plot MAE scores on the holdout test set as red bars
bars2 = ax.bar(x + bar_width / 2, holdout_mae_scores, width=bar_width, label='Holdout Set', color='red', alpha=0.7)

# Add a horizontal line for the best holdout score
ax.axhline(best_holdout_score, color='blue', linestyle='--', linewidth=2, label=f'Best Holdout Score: {best_holdout_score:.4f}')

# Annotate each bar with its value at the bottom
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.4f}',
                    xy=(bar.get_x() + bar.get_width() / 2, 0),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')  # Align annotation to the bottom of the bar

# Set plot labels and title
ax.set_xlabel('Model', fontsize=12)
ax.set_ylabel('Mean Absolute Error', fontsize=12)
ax.set_title('Comparison of Mean Absolute Error on Validation and Holdout Sets', fontsize=14)

# Set x-tick labels
ax.set_xticks(x)
ax.set_xticklabels(model_names)

# Add a legend to distinguish between validation and holdout scores, error bars, and the best holdout score line
ax.legend(loc='upper right')

# Add gridlines for better readability
ax.grid(True, axis='y', linestyle='--', alpha=0.5)

plt.savefig('Overview_MAE.png')

# Show the plot
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Model names
model_names = ['Linear Regression', 'Random Forest', 'XGBoost', 'KNN']

# MSE scores on the validation set
mean_mse_scores = [0.1417, 0.1159, 0.1165, 0.1355]

# MSE scores on the holdout test set
holdout_mse_scores = [0.1462, 0.1157, 0.1153, 0.1299]

# Standard deviation of the cross-validation scores for each model
std_devs = [0.01, 0.02, 0.02, 0.015]

# Best holdout score and its model (not provided)
best_holdout_score = min(holdout_mse_scores)
best_model_index = holdout_mse_scores.index(best_holdout_score)

# Create a figure and axis for the plot
sns.set(style='whitegrid')
fig, ax = plt.subplots(figsize=(10, 6))

# Define the bar width and x-axis positions
bar_width = 0.35
x = np.arange(len(model_names))

# Plot mean MSE scores (cross-validation) with error bars as green bars
bars1 = ax.bar(x - bar_width / 2, mean_mse_scores, width=bar_width, yerr=std_devs, label='Validation Set (Error Bars: Std. Dev)', color='green', alpha=0.7, capsize=5)

# Plot MSE scores on the holdout test set as red bars
bars2 = ax.bar(x + bar_width / 2, holdout_mse_scores, width=bar_width, label='Holdout Set', color='red', alpha=0.7)

# Add a horizontal line for the best holdout score
ax.axhline(best_holdout_score, color='blue', linestyle='--', linewidth=2, label=f'Best Holdout Score: {best_holdout_score:.4f}')

# Annotate each bar with its value at the bottom
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.4f}',
                    xy=(bar.get_x() + bar.get_width() / 2, 0),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')  # Align annotation to the bottom of the bar

# Set plot labels and title
ax.set_xlabel('Model', fontsize=12)
ax.set_ylabel('Mean Squared Error', fontsize=12)
ax.set_title('Comparison of Mean Squared Error on Validation and Holdout Sets', fontsize=14)

# Set x-tick labels
ax.set_xticks(x)
ax.set_xticklabels(model_names)

# Add a legend to distinguish between validation and holdout scores, error bars, and the best holdout score line
ax.legend(loc='upper right')

# Add gridlines for better readability
ax.grid(True, axis='y', linestyle='--', alpha=0.5)

plt.savefig('Overview_MSE.png')

# Show the plot
plt.show()